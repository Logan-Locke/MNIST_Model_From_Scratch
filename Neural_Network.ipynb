{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "k2gbDgLutnbz",
    "outputId": "b4453830-177e-411a-86e8-96ad6cc52f9a",
    "ExecuteTime": {
     "end_time": "2023-06-01T00:50:48.786409Z",
     "start_time": "2023-06-01T00:50:40.227526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7   \n0      1       0       0       0       0       0       0       0       0  \\\n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779   \n0       0  ...         0         0         0         0         0         0  \\\n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 785 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('Data/train.csv') # Loads in the training data\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# **Introduction**\n",
    "I started constructing this notebook after stumbling upon a video made by Samson Zhang. He explained how learning the foundational concepts of a neural network without the assistance of popular libraries/frameworks such as TensorFlow, PyTorch, and Keras enabled him to gain a more thorough understanding of machine learning. As I tried to understand what was happening in the code, I was taken down rabit hole after rabit hole searching YouTube and forums for the explanations. That's when I decided to organize everything I was learning.\n",
    "\n",
    "Since this notebook elaborates on concepts that I did not initially grasp, some of its contents may be quite easy to understand, while others that I did not cover in-depth may be challenging. In such cases, I challenge you to learn using the many available resources. This notebook is tailored towards someone with a surface-level understanding of calculus, linear algebra, and python. Anyone who meets those requirements should be able to follow along and understand this simple neural network using the MNIST digits dataset!\n",
    "\n",
    "This notebook was inspired by and modeled after one created by Samson Zhang, found [here](https://www.kaggle.com/code/wwsalmon/simple-mnist-nn-from-scratch-numpy-no-tf-keras/notebook)\n",
    "\n",
    "The MNIST dataset is from Kaggle, found [here](]https://www.kaggle.com/c/digit-recognizer)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Resource Recommendations**\n",
    "Here are some of the resources I have found the most helpful. Some have unique styles, or are a part of a longer series, so find the one that most suits you!\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "### **Machine Learning/Neural Networks**\n",
    "[\"Why Neural Networks can learn (almost) anything\"](https://www.youtube.com/watch?v=0QczhVg5HaI) - Emergent Garden\n",
    "\n",
    "[Neural Networks (Series)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) - 3Blue1Brown\n",
    "\n",
    "[Neural Networks/Deep Learning (Series)](https://www.youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1) - StatQuest with Josh Starmer\n",
    "\n",
    "[Machine Learning (Series)](https://www.youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF) - StatQuest with Josh Starmer\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Mathematics**\n",
    "[Essence of Linear Algebra (Series)](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) - 3Blue1Brown\n",
    "\n",
    "[Essence of Calculus (Series)](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) - 3Blue1Brown\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# **Definitions**\n",
    "These are some definitions that are either helpful to have as a reference or not covered in this notebook.\n",
    "\n",
    "*All terms are described in the context of machine learning and neural networks. In other applications, these definitions may differ.*\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Arrays**\n",
    "The dimensionality of an array in the context of data structures can be different from the dimensionality of the space it's used to represent. For example, a \"2-D vector\" is a 1-D collection of 2 values that represents a point or direction in two-dimensional space. Additionally, a \"1-D Matrix\" is considered a vector. The dimensionality of the data structures never changes, but the dimensionality of its contents can.\n",
    "\n",
    "**Scalar**\n",
    "0-Dimensional single, real or complex number.\n",
    "\n",
    "**Vector**\n",
    "1-Dimensional array of numbers that can be represented as either a row or a column.\n",
    "\n",
    "**Matrix**\n",
    "2-Dimensional array of numbers where each row or column represents an array.\n",
    "\n",
    "**Tensor**\n",
    "N-Dimensional array of numbers, known as \"N-Order\" tensor. Scalars (0-D tensors), vectors (1-D tensors), and matrices (2-D tensors) are specific types of tensors.\n",
    "\n",
    "<br>\n",
    "<img src=\"Images/Arrays.jpeg\" alt=\"Arrays\" width=\"1200\">\n",
    "\n",
    "Image source: Jovian.com\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Components**\n",
    "**Training Set**\n",
    "Usually the largest set in a given dataset that is used to train the model and adjust parameters.\n",
    "\n",
    "**Validation Set**\n",
    "Also known as the \"dev set\", this is a subset of the training set that is used to evaluate performance and tune hyperparameters.\n",
    "\n",
    "**Testing Set**\n",
    "A set used to assess the final performance and generalization capability of a model. It is kept separate from the training and validation sets and unseen to the model to ensure uninfluenced decisions.\n",
    "\n",
    "**Node/Neuron**\n",
    "The fundamental of a neural network that takes inputs, applies weights and biases, performs computations, and produces outputs.\n",
    "\n",
    "**Regularization Strength**\n",
    "A value used to prevent overfitting that adds a penalty to the loss function to encourage simpler representations. If it's set too high, it could lead to underfitting and poor results. If it's set too low, it could lead to overfitting and poor results.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Miscellaneous**\n",
    "**Overfitting**\n",
    "A scenario that occurs when a model is overly complex, which leads to the model learning randomness and not underlying patterns. It will have good performance on training data at the expense of good performance on the testing data.\n",
    "\n",
    "**Underfitting**\n",
    "A scenario that occurs when a model is not complex enough, which leads to the model developing little capacity to capture patterns. It will have poor performance on both the training and testing sets.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# **Overview**\n",
    "The data we'll be working with is the MNIST dataset. It consists of tens of thousands handwritten images of digits. Thankfully, we don't have to worry about image processing because the images have been converted into values inside a .csv file. Each \"image\" now consists of a 28x28 grid of values (784 total), which has been transformed into a one-dimensional array. Each value is on a scale from 0 to 255, with 0 being completely black and 255 being completely white. By dividing these values by 255, we can normalize them to be on a scale from 0 to 1.\n",
    "\n",
    "<br>\n",
    "<img src=\"Images/Digits_Example.jpeg\" alt=\"Digits Example\" width=\"1200\">\n",
    "\n",
    "Image source: medium.com\n",
    "<br>\n",
    "\n",
    "The dataset is split into two files: the training data and the testing data. The training data is much larger than the testing data because our model will need a lot of information to try to correctly identify the handwritten digits in the testing data. These two datasets are completely disconnected from each other. The testing data will only be introduced to the model after we are completely done with training. From there, the model no longer updates its parameters, and it remains the same throughout the testing data.\n",
    "\n",
    "There is one important distinction between the training and testing data, however. Each image in the training data is accompanied by a label that represents what digit that image represents. Therefore, each array of image data in the training data is actually 785 values (we'll separate the labels from each vector later). Now, we have tens of thousands arrays of length 785 to run through our model!\n",
    "\n",
    "This network will have three total layers: an input layer, a hidden layer, and an output layer.\n",
    "\n",
    "<br>\n",
    "<img src=\"Images/Network_Layers.jpeg\" alt=\"Network Layers\" width=\"1200\">\n",
    "\n",
    "Image source: medium.com\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Data Handling**\n",
    "Since each image is represented by a vector with 784 values, and we have tens of thousands of images, performing operations on these images would require a large amount of computational power. Thankfully, we can take advantage of vectorized calculations by stacking the vectors together into one matrix to increase efficiency. Vectorized calculations involve performing operations on entire arrays rather than individual elements, which significantly speeds up computation time. Some examples of vectorized calculations we'll implement are dot products, matrix multiplication, and element-wise operations.\n",
    "\n",
    "In most applications, these vectors would be represented as rows of the matrix with dimensions $m \\times n$ where $m$ is the number of training examples and $n$ is the number of input features (784). In linear algebra, $m$ refers to the number of rows and $n$ refers to the number of columns. Another alteration we're going to make to our data is transposing it. This is common practice in machine learning because it allows our data to adhere to the conventions of linear algebra and also specific algorithms. Now, our matrix will have dimensions $n \\times m$, with each column corresponding to a training example and each row a training feature. A transposed matrix is denoted by the addition of a superscripted $T$.\n",
    "\n",
    "<br>\n",
    "<img src=\"Images/Transpose.jpeg\" alt=\"Transpose\" width=\"1200\">\n",
    "\n",
    "Image source: cuemath.com\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Input Layer**\n",
    "The input layer is the very first layer of our neural network. Essentially, this layer's only purpose is to represent the input data, or input features. This layer is often disregarded when counting how many layers a network has, so ours would only be a two-layer neural network.\n",
    "\n",
    "Throughout the entire process of training our model, this layer will always have 784 nodes, or neurons. You can think of these as a unit of the neural network that holds and processes data. Individually, nodes don't do much. When you connect them to other nodes and apply functions to them however, that's how we make a neural network.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Hidden Layer**\n",
    "\n",
    "The hidden layer is the first layer of our network that performs some action on the data. The amount of nodes in any hidden layer is arbitrary, it's entirely up to you how many you want to incorporate. You can increase or decrease the amount as much as you like depending on the performance of the model. In fact, most neural networks have multiple hidden layers, but we're using one to keep things simple.\n",
    "\n",
    "Since no values are associated with the nodes in the hidden layer, now we have to perform some operation on the data. We're now going to introduce weights and biases to our model, each of which starts off as completely randomized values. These two parameters are the primary parameters that are updated during the training process.\n",
    "\n",
    "We're also going to apply an activation function to the values associated with the hidden layer. The choice of activation function is another element of a neural network that can be altered. For this, we're going to use what's called a rectified linear unit (ReLU) activation function. ReLU will take in  \"weighted sums\", \"pre-activation values\", \"logits\" and perform an operation on each of those to return \"activations\", or \"activation values\".\n",
    "\n",
    "After ReLU is has been used, these values are ready to move onto the next layer.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Output Layer**\n",
    "\n",
    "Our output layer will have 10 nodes, one for each possible digit. We will introduce another set of weights and biases connecting the hidden layer to the output layer. Once again, we will apply an activation function to the pre-activation values specific to this layer. However, the output layer is going to use a different one than ReLU called a softmax function.\n",
    "\n",
    "A softmax function will transform our pre-activation values into an array of probabilities that sums up very nicely to 1. Essentially, each probability value of the output layer represents how confident the model is that a certain image is a particular digit. Now, we have our final output.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "id": "TQTOzEwgtnb1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset Initialization\n",
    "\n",
    "m: rows\n",
    "n: columns\n",
    "\"\"\"\n",
    "data = np.array(data)\n",
    "m, n = data.shape # Assigns dimension values to m and n\n",
    "np.random.shuffle(data) # Shuffles the data to prevent overfitting\n",
    "\n",
    "\"\"\"\n",
    "Validation Dataset Initialization\n",
    "\n",
    "X: Input features of the validation dataset that are used to make predictions\n",
    "Y: Target variables of the validation dataset that are the expected output of the input data\n",
    "\"\"\"\n",
    "validation_data = data[0:1000].T # Grabs the first 1000 columns of the dataset\n",
    "Y_validation = validation_data[0] # Creates a vector of the target variables, or \"correct digits\"\n",
    "X_validation = validation_data[1:, :]\n",
    "X_validation= X_validation / 255.0\n",
    "\n",
    "\"\"\"\n",
    "Training Dataset Initialization\n",
    "\n",
    "X: Input features of the training dataset that are used to make predictions\n",
    "Y: Target variables of the training dataset that are the expected output of the input data\n",
    "\"\"\"\n",
    "training_data = data[1000:m].T\n",
    "Y_training = training_data[0]\n",
    "X_training = training_data[1:n]\n",
    "X_training = X_training / 255.0\n",
    "_,m_training = X_training.shape # Retrieves the size of X_training and assigns the amount of columns to m_training because the amount is not initially known, unlike in validation_data"
   ],
   "metadata": {
    "id": "AiOGvYJftnb3",
    "ExecuteTime": {
     "end_time": "2023-06-01T00:50:50.513634Z",
     "start_time": "2023-06-01T00:50:48.722414Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# **Transposing**\n",
    "It may be quite confusing to visualize why we transposed matrices and how transposing actually works. In order to clear some potential confusion, we'll work through an example very similar to the operations that were just performed in the above code. Note, $y_i$ will be used to represent the true labels of our training data (the ones we separated from the rest). This notation will be expanded on in the future, but I thought it would help increase clarity.\n",
    "\n",
    "This is a matrix $A$ representing the training data, with 5 rows and 3 columns:\n",
    "$$A = \\begin{bmatrix}\n",
    "{y}_0 & 1 & 2 \\\\\n",
    "{y}_1 & 3 & 4 \\\\\n",
    "{y}_2 & 5 & 6 \\\\\n",
    "{y}_3 & 7 & 8 \\\\\n",
    "{y}_4 & 9 & 10\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "For our validation data $B$, extract the first 2 rows and transpose the result as $B^{T}$. We'll use validation_data = data[0:2].T:\n",
    "\n",
    "$$B = \\begin{bmatrix}\n",
    "{y}_0 & 1 & 2 \\\\\n",
    "{y}_1 & 3 & 4\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$B^{T} = \\begin{bmatrix}\n",
    "{y}_0 & {y}_1 \\\\\n",
    "1 & 3 \\\\\n",
    "2 & 4\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Next, retrieve the true label vector $B_{Y}$ and then the input features matrix $B_{X}$. We'll use Y_validation = validation_data[0] and X_validation = validation_data[1:3]:\n",
    "\n",
    "$$B^{T}_{Y} = \\begin{bmatrix}\n",
    "{y}_0 & {y}_1 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$B^{T}_{X} = \\begin{bmatrix}\n",
    "1 & 3 \\\\\n",
    "2 & 4\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "As you can see, we've separated the true labels from the data array. Now, we can use the true labels later on when we need to inform the model on whether or not it correctly predicted the digit.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# **Parameters**\n",
    "Recall, the layers of our neural network are 0-indexed where the current layer is represented by $k$. The weight matrices and bias vectors utilize this notation.\n",
    "\n",
    "As mentioned previously, weights and biases are the primary parameters that are adjusted by the model.\n",
    "\n",
    "Every node in the input layer has a weight value connecting to every node in the hidden layer (that's a lot of connections!). This is where the \"network\" part of neural network is emphasized.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Weight**\n",
    "Weights are learnable parameters that determine the strength of the connections between nodes in adjacent layers. Each weight value in the matrix influences the contribution, or importance, of the connection between a node in the previous layer $k-1$ to a node in the current layer $k$. These values are used to calculate the pre-activation values.\n",
    "\n",
    "$$W^{[k]} = \\begin{bmatrix}\n",
    "w_{0,0} & w_{0,1} & \\dots & w_{0,n} \\\\\n",
    "w_{1,0} & w_{1,1} & \\dots & w_{1,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{m,0} & w_{m,1} & \\dots & w_{m,n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Each row of $W^{[k]}$ represents the connections between all nodes of the previous layer $k-1$ to a particular node in the current layer $k$, while each column of $W^{[k]}$ represents the connections between a particular node of the previous layer $k-1$ to all nodes in the current layer $k$.\n",
    "\n",
    "The weight $w_{m,n}$ represents the weight value for the connection from the $n-th$ node in the previous layer $k-1$ to the $m-th$ node in the current layer $k$. In the first weight matrix $W^{[1]}$, $m=10$ and $n=784$ which results in a matrix with 7,840 elements. In the second weight matrix $W^{[2]}$, $m=10$ and $n=10$ which results in a matrix with 100 elements. Note, $m$ and $n$ are serving two purposes in this context. In $W^{[k]}$, $m$ represents both the row index and the amount of rows, while $n$ represents both the column index and the amount of columns. $W^{[k]}$ is of size $m \\times n$.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Bias**\n",
    "Biases are a set of learnable parameters that are added to thw pre-activation values, just before applying an activation function. Bias is used to shift the axis of an activation function to ensure certain input values (especially those will a value of 0, which can still be helpful information to guide the model) don't get cut off when calculating predictions. It also ensures that all variations of input values are captured. For example, a digit shifting to the left will still be identified by the model, even though it's in an entirely different position than it was previously.\n",
    "\n",
    "$$b^{[k]} = \\begin{bmatrix}\n",
    "b_0  \\\\\n",
    "b_1  \\\\\n",
    "\\vdots  \\\\\n",
    "b_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The bias $b_n$ represents the bias value of the $n-th$ node in the current layer $k$.\n",
    "\n",
    "Both bias vectors in our neural network, $b^{[1]}$ and $b^{[2]}$, contain 10 elements, one for each node in their respective layers.\n",
    "\n",
    "<br>\n",
    "<img src=\"Images/Bias_ReLU.jpeg\" alt=\"Bias and ReLU\" width=\"1200\">\n",
    "\n",
    "Image source: turing.com\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Rty6DJ8otnb4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes the weight (m, n) and bias (m, 1) arrays with random values between 0 and 1. Then, To center the distribution around 0, 0.5 is subtracted\n",
    "\n",
    "    m=neurons in current layer\n",
    "    n=neurons in previous layer\n",
    "\n",
    "    Returns:\n",
    "        W1: Matrix of randomized weight values for the input features to the hidden layer\n",
    "        b1: (Vector of randomized bias values for the input layer to the hidden layer\n",
    "        W2: Matrix of randomized weight values for the hidden layer to the output layer\n",
    "        b2: Vector of randomized bias values for the hidden layer to the output layer\n",
    "    \"\"\"\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2"
   ],
   "metadata": {
    "id": "kbR4s0jqtnb4",
    "ExecuteTime": {
     "end_time": "2023-06-01T00:50:50.517053Z",
     "start_time": "2023-06-01T00:50:50.514069Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# **Layer Operations**\n",
    "This is where we actually start getting into what calculations happen in our network! It's relatively simple, but there are a lot of moving parts, so ensure you understand each before moving on.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Pre-Activation Values**\n",
    "Pre-activation values, also called weighted sums, are calculated by computing the dot product of a weight matrix $W^{[k]}$ and input values $X$ or $A^{[1]}$, depending on which layer the pre-activation values are calculated, plus a bias value $b^{[k]}$. In our network, each set of pre-activation values results in a vector with 10 elements, equal to the amount of nodes in each layer.\n",
    "\n",
    "<br>\n",
    "<img src=\"Images/Dot_Product.jpeg\" alt=\"Dot Product\" width=\"1200\">\n",
    "\n",
    "Image source: algebra1course.wordpress.com\n",
    "<br>\n",
    "\n",
    "*Note, the term \"input values\" generally refers to values that are used as an input to a function (in this case, our pre-activation values calculation). It's different from the term \"input features\", which exclusively refers to the values in the input layer that represent the image data from our dataset.*\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Activation Values**\n",
    "As the name might suggest, these are the values obtained after applying an activation function, and the final operation performed in their respective layer. Before we demonstrate how this is done, we need to provide the activation functions we'll be using in this neural network.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **ReLU**\n",
    "Rectified Linear Unit (ReLU) is an activation function that returns the input value $x$ if $x>0$, and zero otherwise. ReLU introduces non-linearity to the network, allowing it to learn and represent complex relationships between inputs and outputs.\n",
    "\n",
    "ReLU function:\n",
    "$$f(x) = max(0, x)$$\n",
    "\n",
    "<br>\n",
    "<img src=\"Images/ReLU.jpeg\" alt=\"ReLU\" width=\"1200\">\n",
    "\n",
    "Image source: vidyasheela.com\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Softmax**\n",
    "The softmax function transforms a vector $Z$ into a probability distribution. Each element in the output vector represents the probability of the input belonging to a particular class. The output vector is one whose elements sum up to 1 and are all non-negative (these are really important properties).\n",
    "\n",
    "Softmax function:\n",
    "$$\\hat{y}_i = \\frac{e^{Z_i}}{\\sum_{j=0}^{C-1} {e^{Z_j}}}$$\n",
    "\n",
    "In this, ${Z_i}$ represents the input vector of pre-activation values, ${e^{Z_i}}$ represents a particular node, $e^{Z_j}$ represents all nodes and $C$ represents the amount of classes. The numerator raises $e$ to the power of some value $Z_i$ of the input vector. The denominator raises $e$ to the power of each $Z_j$, which is $C$ classes, and then sums up all these terms.\n",
    "\n",
    "<br>\n",
    "<img src=\"Images/Softmax.jpeg\" alt=\"Softmax\" width=\"1200\">\n",
    "\n",
    "Image source: vitalflux.com\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# **Forward Propagation**\n",
    "As the name suggests, our neural network will do two separate passes: a forwards pass (forward propagation) and a backwards pass (backpropagation). This section will explain what happens during forward propagation.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Hidden Layer**\n",
    "The first operation in this layer is to calculate the pre-activation values by inserting our randomized weight and bias arrays, $W^{[1]}$ and $b^{[1]}$, into the following function:\n",
    "$$Z^{[1]} = W^{[1]} \\cdot X + b^{[1]}$$\n",
    "\n",
    "In this, $Z^{[1]}$ represents our pre-activation values and $X$ represents our input features. $Z^{[1]}$ is a new matrix of dimensions $10\\times m$, where $m$ represents the amount of training examples. The entire matrix represents carrying out the first step of forward propagation for all training examples at the same time (this approach was intentionally chosen, it will be expanded on in the \"gradient descent\" section).\n",
    "\n",
    "Next, we need to apply an activation function to $Z^{[1]}$. In this model, our chosen activation function for this layer is ReLU. From $Z^{[1]}$, we'll calculate $A^{[1]}$, which represents the values of our nodes in the hidden layer after applying the activation function:\n",
    "$$A^{[1]} = \\text{ReLU}(Z^{[1]})$$\n",
    "\n",
    "Now, we can proceed to calculating the values of the output layer.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Output Layer**\n",
    "The same methodology as the hidden layer applies here. We take in input values from a previous layer, calculate the pre-activation values, and then calculate our activation values.\n",
    "\n",
    "First, we'll calculate $Z^{[2]}$:\n",
    "$$Z^{[2]} = W^{[2]} \\cdot A^{[1]} + b^{[2]}$$\n",
    "\n",
    "Then, we'll apply an activation function to $Z^{[2]}$. For the output layer, we've chosen the softmax activation function:\n",
    "$$A^{[2]} = \\text{softmax}(Z^{[2]})$$\n",
    "\n",
    "That's forward propagation! We went from input features, to hidden layer activation values, and then to output layer activation values.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "id": "AS1sm9ibtnb4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    \"\"\"\n",
    "    Implements the rectified linear unit (ReLU) activation function\n",
    "\n",
    "    Args:\n",
    "        Z: Pre-activation values\n",
    "\n",
    "    Returns:\n",
    "        Vector of size Z where each element is represented as the maximum of that particular value and 0\n",
    "    \"\"\"\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Implements the softmax activation function\n",
    "\n",
    "    Args:\n",
    "        Z: Pre-activation values\n",
    "\n",
    "    Returns:\n",
    "        Vector of probability values between 0 and 1\n",
    "    \"\"\"\n",
    "    return np.exp(Z) / sum(np.exp(Z))\n",
    "\n",
    "def forward_propagation(W1, b1, W2, b2, X):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation function\n",
    "\n",
    "    Args:\n",
    "        W1: Matrix of weight values for the input layer and the hidden layer\n",
    "        b1: Vector of bias values for the input layer to the hidden layer\n",
    "        W2: Matrix of weight values for the hidden layer and the output layer\n",
    "        b2: Vector of bias values for the hidden layer to the output layer\n",
    "        X: Matrix of input features\n",
    "\n",
    "    Returns:\n",
    "        Z1: Vector of the dot product of W1 and X, plus the bias values b1\n",
    "        A1: Vector of activation values by applying ReLU to Z1\n",
    "        Z2: Vector of the dot product of W2 and A1, plus the bias values b2\n",
    "        A2: Vector of activation values by applying softmax to Z2\n",
    "    \"\"\"\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ],
   "metadata": {
    "id": "pyce7amctnb5",
    "ExecuteTime": {
     "end_time": "2023-06-01T00:50:50.517538Z",
     "start_time": "2023-06-01T00:50:50.515315Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# **Backpropagation**\n",
    "Backpropagation, or \"backprop\", is the derivative of the loss function with respect to the softmax output. The \"loss\" (calculated by finding the difference between the model's predictions and the true targets) is also commonly referred to as the \"cost\". Our objective for backprop is to minimize the cost of our neural network by manipulating the weights and biases. When the cost is lower, our model is more accurate.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "## One-Hot Encoding\n",
    "One-hot encoding is a method that converts discrete variables into numeric data. In our neural network, this is used on the labels for each image (0-9), which is a discrete variable. Using one-hot encoding, these labels are converted into a binary vector of length 10 (one for each digit). Each position in the vector corresponds to a digit, and the position representing the correct digit is marked with a 1, while all other positions are marked with a 0.\n",
    "\n",
    "We'll show an example of this in the next section.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Loss Function**\n",
    "A common choice for the loss function given the implementation of softmax is called \"cross-entropy loss\" because of the two properties of a softmax output vector mentioned earlier. Furthermore, since we're dealing with a multi-class classification problem (digits 0-9), cross-entropy loss is a great choice.\n",
    "\n",
    "Cross-entropy loss function:\n",
    "$$L_{CE} = -\\sum_{i=0}^{C-1} y_i \\cdot \\log(\\hat{y}_i)$$\n",
    "\n",
    "Note, in the context of machine learning, natural log is typically used when referring to \"log\", which derives nicely to $\\frac{1}{x}$. Try not to confuse the log used in cross-entropy for the log traditionally used in other applications.\n",
    "\n",
    "In this, ${y}_i$ is the true label (0 or 1) from one-hot encoding, ${\\hat{y}_i}$ is the probability confidence from softmax. Just as in softmax, $C$ represents the amount of classes.\n",
    "\n",
    "For example, $\\hat{y}$ could look like:\n",
    "$$\\begin{bmatrix} 0.02, \\ 0.00, \\ 0.03, \\ 0.05, \\ 0.01, \\ 0.06, \\ \\textbf{0.72}, \\ 0.09, \\ 0.00, \\ 0.02 \\end{bmatrix}$$\n",
    "\n",
    "If the true label for this example was 6, $y$ would look like:\n",
    "$$\\begin{bmatrix} 0, \\ 0, \\ 0, \\ 0, \\ 0, \\ 0, \\ \\textbf{1}, \\ 0, \\ 0, \\ 0 \\end{bmatrix}$$\n",
    "\n",
    "As you can see, $y_i = 0$ for all $i$ except the correct label. Consequently, the only $\\hat{y}$ value that influences the cost is correct label.\n",
    "\n",
    "To visualize this, the loss function of our example would look like:\n",
    "$$L_{CE} = - (y_0 \\cdot \\log(\\hat{y}_0)) + \\dots + (y_6 \\cdot \\log(\\hat{y}_6)) + \\dots + (y_9 \\cdot \\log(\\hat{y}_9))$$\n",
    "$$= - (0 \\cdot \\log(0.02) + \\dots + (1 \\cdot \\log(0.72)) + \\dots + (0 \\cdot \\log(0.02))$$\n",
    "$$= -\\log(0.72) \\approx \\textbf{0.143}$$\n",
    "\n",
    "Since the objective is to minimize the cost (get closer to 0), the model would have demonstrated better performance if ${\\hat{y}_6}$ were larger:\n",
    "$$L_{CE} = -\\log(\\hat{y}_6) = -\\log(0.86) \\approx \\textbf{0.060}$$\n",
    "\n",
    "The closer ${\\hat{y}}$ is to 1, the closer the cost is to 0.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# **Derivative Calculations**\n",
    "This is the not so fun part. If you're not interested in learning how we were able to find that $\\frac{\\delta L_{CE}}{\\delta {Z_i}} = {\\hat{y}_i} -{y_i}$, feel free to skip over this section. With that said, let's continue!\n",
    "\n",
    "Our objective is to find the derivative of the loss function $L_{CE}$ with respect to the inputs of the final layer $Z_i$.\n",
    "\n",
    "You may be familiar with the \"standard\" chain rule:\n",
    "$${\\frac{dz}{dt} = \\frac{dz}{dx} \\cdot \\frac{dx}{dt}}$$\n",
    "\n",
    "However, we will need to tweak it to work for the cross-entropy loss function. This is because the derivative of the cross-entropy loss function with respect to the inputs of the softmax function involves multiple variables. Each output of the softmax function $\\hat{y}_i$ depends on all the inputs $Z_j$, and the loss function depends on all the outputs $\\hat{y}_i$. Changes in any input $Z_i$ can affect all the outputs $\\hat{y}_i$, and consequently, the loss function depends on all the outputs collectively. We can update our chain rule formula to:\n",
    "$${\\frac{dz}{dt} = \\sum_{i=0}^{C-1} \\frac{\\delta z}{\\delta x_i} \\cdot \\frac{dx_i}{dt}}$$\n",
    "\n",
    "Using this updated chain rule, we can define the expression we will start calculating, piece by piece. This expression is the derivative of the loss function $L_{CE}$ with respect to the inputs ${Z_j}$:\n",
    "$$\\frac{\\delta L_{CE}}{\\delta {Z_j}} = \\sum_{i} \\frac{\\delta L_{CE}}{\\delta {\\hat{y}_i}} \\cdot \\frac{\\delta {\\hat{y}_i}}{\\delta {Z_j}}$$\n",
    "\n",
    "In this, $\\frac{\\delta L_{CE}}{\\delta \\hat{y}_i}$ represents the derivative of cross-entropy $L_{CE}$ with respect to the softmax output ${\\hat{y}_i}$, and $\\frac{\\delta \\hat{y}_i}{\\delta {Z_j}}$ represents the derivative of the softmax output ${\\hat{y}_i}$ with respect to the inputs $Z_j$.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Deriving Cross-Entropy**\n",
    "To find $\\frac{\\delta L_{CE}}{\\delta \\hat{y}_i}$, let's consider an example with three classes: Class 0, Class 1, and Class 3. The softmax outputs for the classes are represented as  $\\hat{y}_0$, $\\hat{y}_1$, and $\\hat{y}_2$, and the true label for the input is represented by a one-hot encoded vector $y = [y_0, y_1, y_2]$. For this, the loss function calculation is:\n",
    "$$L_{CE} = -\\sum_{i=0}^{2} y_i \\cdot \\log(\\hat{y}_i)$$\n",
    "$$= -(y_0 \\cdot \\log(\\hat{y}_0)) + (y_1 \\cdot \\log(\\hat{y}_1)) + (y_2 \\cdot \\log(\\hat{y}_2))$$\n",
    "\n",
    "All $y_i = 0$ except for the correct class, where $y_i = 1$. For example, let's say Class 1 is the correct class, then $y = [0, 1, 0]$. Now, let's calculate the derivative of $L_{CE}$ with respect to $\\hat{y}_0$, $\\hat{y}_1$, and $\\hat{y}_2$:\n",
    "$$\\frac{\\delta L_{CE}}{\\delta \\hat{y}_0} = -{\\frac{0}{\\hat{y}_0}} = 0$$\n",
    "$$\\frac{\\delta L_{CE}}{\\delta \\hat{y}_1} = -{\\frac{1}{\\hat{y}_1}} $$\n",
    "$$\\frac{\\delta L_{CE}}{\\delta \\hat{y}_2} = -{\\frac{0}{\\hat{y}_2}} = 0$$\n",
    "\n",
    "When $i$ equals the index of the correct class: $$\\frac{\\delta L_{CE}}{\\delta \\hat{y}_i} = -\\frac{1}{\\hat{y}_i}$$\n",
    "\n",
    "When $i$ does not equal the index of the correct class: $$\\frac{\\delta L_{CE}}{\\delta \\hat{y}_i} = 0$$\n",
    "\n",
    "This reflects the fact that changing the prediction $\\hat{y}_i$ for a class which is not the correct class doesn't affect the cost. From this, we can conclude that:\n",
    "$$\\frac{\\delta L_{CE}}{\\delta \\hat{y}_i} = -{\\frac{y_i}{\\hat{y}_i}}$$\n",
    "\n",
    "Remember, this is only the first piece of the $\\frac{\\delta L_{CE}}{\\delta {Z_i}}$ expression, but we're halfway there!\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Deriving ReLU**\n",
    "This derivative is quite simple, just like calculating ReLU itself. For $x > 0$, the derivative is 1 because the function is just $y = x$. For $x \\leq 0$, the derivative is 0 because the function is constant ($y = 0$).\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Deriving Softmax**\n",
    "Quotient rule:\n",
    "$$\\frac{dy}{dx} = \\frac{v{\\frac{du}{dx}} - u{\\frac{dv}{dx}}}{v^2}$$\n",
    "\n",
    "Softmax function:\n",
    "$$\\hat{y}_i = \\frac{e^{Z_i}}{\\sum_{j=0}^{C-1} e^{Z_j}}$$\n",
    "\n",
    "To get the derivative of softmax with respect to the input, we need to consider two cases: when $i = j$ and when $i \\neq j$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "### **When i = j**\n",
    "This is the case where $i$ and $j$ are the same. We're calculating the derivative of the softmax output $\\hat{y}_i$ with respect to the input $Z_i$.\n",
    "\n",
    "Now, we need to use the quotient rule, but for simplicity, let's say that when $\\sum$ is used in these following expressions, it represents $\\sum_{j=0}^{C-1}$:\n",
    "$$\\frac{\\delta \\hat{y}_i}{\\delta {Z_i}} = \\frac{e^{Z_i} \\sum e^{Z_j} - e^{Z_i} e^{Z_i}} {({\\sum e^{Z_j}})^{2}}$$\n",
    "\n",
    "Next, let's expand the denominator to make our expression:\n",
    "$$\\frac{\\delta \\hat{y}_i}{\\delta {Z_i}} = \\frac{e^{Z_i} \\sum e^{Z_j} - e^{Z_i} e^{Z_i}} {{\\sum e^{Z_j}}{\\sum e^{Z_j}}}$$\n",
    "\n",
    "Then, factor out $e^{Z_i}$ from the numerator to yield:\n",
    "$$\\frac{\\delta \\hat{y}_i}{\\delta {Z_i}} = \\frac{e^{Z_i} (\\sum e^{Z_j} - e^{Z_i})} {{\\sum e^{Z_j}}{\\sum e^{Z_j}}}$$\n",
    "\n",
    "To make the next step more intuitive, we can represent softmax as $\\hat{y}_i$ = $\\frac{e^{Z_i}}{\\sum e^{Z_j}}$.\n",
    "\n",
    "As you can see, since we have $\\frac{e^{Z_i}}{\\sum e^{Z_j}}$ present in our expression as the first terms of the numerator and denominator, we can pull them out as $\\hat{y}_i$:\n",
    "$$\\frac{\\delta \\hat{y}_i}{\\delta {Z_i}} = \\hat{y}_i \\frac{\\sum e^{Z_j} - e^{Z_i}} {{\\sum e^{Z_j}}}$$\n",
    "\n",
    "Now, split the fraction into two parts. This results in:\n",
    "$$\\frac{\\delta \\hat{y}_i}{\\delta {Z_i}} = \\hat{y}_i \\frac{\\sum e^{Z_j}} {\\sum e^{Z_j}} - \\frac{e^{Z_i}} {{\\sum e^{Z_j}}}$$\n",
    "\n",
    "After cancelling out the ${\\sum e^{Z_j}}$ terms and identifying another $\\hat{y}_i$, this simplifies to:\n",
    "$$\\frac{\\delta \\hat{y}_i}{\\delta {Z_i}} = \\hat{y}_i (1 - \\hat{y}_i)$$\n",
    "\n",
    "That's it! That's the derivative of softmax when $i = j$. Now, we have to find when $i ≠ j$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **When i ≠ j**\n",
    "This is the case where $i$ and $j$ are not the same. We're calculating the derivative of the softmax output $\\hat{y}_i$ with respect to the input $Z_j$.\n",
    "\n",
    "Just as in the case when $i = j$, use the quotient rule and understand $\\sum$ represents $\\sum_{i=0}^{C-1}$:\n",
    "$$\\frac{\\delta \\hat{y}_i}{\\delta {Z_j}} = \\frac{0 \\cdot \\sum e^{Z_j} - e^{Z_i} e^{Z_j}} {({\\sum e^{Z_j}})^{2}}$$\n",
    "\n",
    "Next, let's expand the denominator to make our expression:\n",
    "$$\\frac{\\delta \\hat{y}_i}{\\delta {Z_j}} =\\frac{-e^{Z_i} e^{Z_j}} {{\\sum e^{Z_j}}{\\sum e^{Z_j}}}$$\n",
    "\n",
    "Next, split the fraction into two parts. This results in:\n",
    "$$\\frac{\\delta \\hat{y}_i}{\\delta {Z_j}} =  -\\frac{e^{Z_i}} {\\sum e^{Z_j}} \\cdot \\frac{e^{Z_j}} {\\sum e^{Z_j}}$$\n",
    "\n",
    "Using the same logic from the $i = j$ case, we can express this as:\n",
    "$$\\frac{\\delta \\hat{y}_i}{\\delta {Z_j}} =  -\\hat{y}_i \\cdot \\hat{y}_j$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Combined Expression**\n",
    "Now, we can combine the two cases together into one, clean expression. Even though both cases will be put together, remember that the first portion represents when $i = j$ and the second portion represents when $i ≠ j$. We'll address this later, but keep this in mind for now. Putting both parts together, we have:\n",
    "$$\\frac{\\delta \\hat{y}_i}{\\delta {Z_j}} =  \\hat{y}_i (1 - \\hat{y}_i) - \\hat{y}_i \\hat{y}_j$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Total Calculation**\n",
    "In this, $i$ is used to index the class for which we're computing the derivative of, while $j$ is used to index over all possible classes in the denominator.\n",
    "\n",
    "Recall the cross-entropy loss function:\n",
    "$$L_{CE} = -\\sum_{i=0}^{C-1} y_i \\cdot \\log(\\hat{y}_i)$$\n",
    "\n",
    "As we found earlier, the derivative of $L_{CE}$ with respect to $\\hat{y}_i$ is:\n",
    "$$\\frac{\\delta L_{CE}}{\\delta \\hat{y}_i} = -\\frac{y_i}{\\hat{y}_i}$$\n",
    "\n",
    "Now, we can use the softmax and cross-entropy derivatives to fill in the expression for the derivative of the loss function $L_{CE}$ with respect to the inputs ${Z_j}$:\n",
    "$$\\frac{\\delta L_{CE}}{\\delta {Z_j}} = \\sum_{i} \\frac{\\delta L_{CE}}{\\delta {\\hat{y}_i}} \\cdot \\frac{\\delta {\\hat{y}_i}}{\\delta {Z_j}}$$\n",
    "$$= -\\sum_{i} \\frac{y_i}{\\hat{y}_i} \\cdot [\\hat{y}_i (1 - \\hat{y}_i) - \\hat{y}_i \\hat{y}_j]$$\n",
    "\n",
    "Remember, we haven't made a notational distinction between the two cases where $i = j$ and $i ≠ j$, so we'll address that now:\n",
    "$$\\frac{\\delta L_{CE}}{\\delta {Z_j}} = -\\sum_{i=j} \\frac{y_i}{\\hat{y}_i} \\cdot \\hat{y}_i (1 - \\hat{y}_i) + \\sum_{i≠j} \\frac{y_i}{\\hat{y}_i} \\cdot \\hat{y}_i \\hat{y}_j$$\n",
    "$$= -\\sum_{i=j} y_i \\cdot (1 - \\hat{y}_i) + \\sum_{i≠j} {y}_i \\hat{y}_j$$\n",
    "$$= -\\sum_{i=j} y_i + y_i \\hat{y}_i + \\sum_{i≠j} {y}_i \\hat{y}_j$$\n",
    "\n",
    "In the left expression, $i = j$. Therefore, we can simplify $\\sum_{i=j} {y}_i$ to ${y}_j$. Also, we are able to use $y_i$ and $y_j$ interchangeably,:\n",
    "$$\\frac{\\delta L_{CE}}{\\delta {Z_j}} = -y_j + y_j \\hat{y}_j + \\sum_{i≠j} {y}_i \\hat{y}_j$$\n",
    "\n",
    "Now, we need to combine the two cases of $i = j$ and $i ≠ j$ together. into one summation:\n",
    "$$\\frac{\\delta L_{CE}}{\\delta {Z_j}} = -y_j + \\sum_{i} {y}_i \\hat{y}_j$$\n",
    "\n",
    "Recall that $y$ is one-hot encoded, so all $y_i$ are 0 except for the correct class, where it equals 1. Because of this, $\\sum_{i} {y}_i \\hat{y}_j$ simplifies to just $1 \\cdot \\hat{y}_j$:\n",
    "$$\\frac{\\delta L_{CE}}{\\delta {Z_j}} = -y_j + 1 \\cdot \\hat{y}_j$$\n",
    "$$= \\hat{y}_j - y_j$$\n",
    "\n",
    "We're done! Now we have the derivative of the loss function with respect to the inputs of softmax. This will be crucial for our backpropagation implementation.\n",
    "\n",
    "This is why cross-entropy and softmax are commonly used in conjunction, the final representation is quite simple!\n",
    "\n",
    "Note, the result of $\\frac{\\delta L_{CE}}{\\delta {Z_j}} = \\sum_{i} \\frac{\\delta L_{CE}}{\\delta {\\hat{y}_i}} \\cdot \\frac{\\delta {\\hat{y}_i}}{\\delta {Z_j}}$ is only explicitly used to calculate the variable $Z^{[2]}$ in the backpropagation function in our code. Although $Z^{[2]}$ is used to calculate other variables, all of this math was mostly for $Z^{[2]}$.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "id": "i4tx8T8qtnb5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def one_hot_encode(Y):\n",
    "    \"\"\"\n",
    "    Implements the one-hot encoding algorithm\n",
    "\n",
    "    Args:\n",
    "        Y: True labels\n",
    "\n",
    "    Returns:\n",
    "        one_hot_Y: Vector of one-hot encoded true labels\n",
    "    \"\"\"\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Z: Pre-activation values\n",
    "\n",
    "    Returns:\n",
    "        1 if x > 0, and 0 if x <= 0\n",
    "    \"\"\"\n",
    "    return Z > 0\n",
    "\n",
    "def backpropagation(Z1, A1, A2, W2, X, Y):\n",
    "    \"\"\"\n",
    "    Initializes the backwards propagation function that calculates gradients (the rate of change of a quantity w.r.t. the parameters) to measure sensitivity to variations. Using these measurements, it will update the parameters to minimize the cost and improve performance\n",
    "\n",
    "    Args:\n",
    "        Z1: Pre-activation values of the hidden layer\n",
    "        A1: Activation values of the first layer after ReLU\n",
    "        A2: Activation values of the output layer after softmax\n",
    "        W2: Weight values of the output layer\n",
    "        X: Input features\n",
    "        Y: True labels\n",
    "\n",
    "    Returns:\n",
    "        dW1: Gradient of the weights of the first layer\n",
    "        db1: Gradient of the biases of the first layer\n",
    "        dW2: Gradient of the weights of the second layer\n",
    "        db2: Gradient of the biases of the second layer\n",
    "    \"\"\"\n",
    "    m = Y.size\n",
    "    one_hot_Y = one_hot_encode(Y)\n",
    "    dZ2 = A2 - one_hot_Y # Derivative of the loss function w.r.t the pre-activation values of the output layer\n",
    "    dW2 = dZ2.dot(A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    dZ1 = W2.T.dot(dZ2) * deriv_ReLU(Z1) # Derivative of the loss function w.r.t. the pre-activation values of the output layer\n",
    "    dW1 = dZ1.dot(X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ],
   "metadata": {
    "id": "sg0WBZl9tnb6",
    "ExecuteTime": {
     "end_time": "2023-06-01T00:50:50.517906Z",
     "start_time": "2023-06-01T00:50:50.515628Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Gradient Descent**\n",
    "\n",
    "To optimize our neural network (minimize the cost) and improve the accuracy of our model, we will now calculate gradient descent, which is an optimization algorithm that finds the parameters that minimize the loss function. It adjusts towards the direction of steepest descent in the loss function gradient. It utilizes the chain rule to compute the gradient of the loss function with respect to each parameter.\n",
    "\n",
    "There are various implementations of gradient descent, but we are implementing what's called \"batch gradient descent\" because it's pretty straightforward. This implementation calculates the average gradient of the loss function over all training examples before updating the parameters (i.e. all images are run forwards and backwards at the same time). It results in accurate, but computationally expensive calculations.\n",
    "\n",
    "The algorithm updates our parameters by subtracting the derivative of the loss function with respect to a particular parameter, from that same parameter. Following this, the formulas are:\n",
    "$$W^{[1]} = W^{[1]} - \\alpha \\cdot \\frac{\\delta L_{CE}}{\\delta W^{[1]}}$$\n",
    "$$b^{[1]} = b^{[1]} - \\alpha \\cdot \\frac{\\delta L_{CE}}{\\delta b^{[1]}}$$\n",
    "$$W^{[2]} = W^{[2]} - \\alpha \\cdot \\frac{\\delta L_{CE}}{\\delta W^{[2]}}$$\n",
    "$$b^{[2]} = b^{[2]} - \\alpha \\cdot \\frac{\\delta L_{CE}}{\\delta b^{[2]}}$$\n",
    "\n",
    "Here, $\\alpha$ is the learning rate. It's a hyperparameter that is explicitly set by the user, rather than one that gradient descent optimizes. The learning rate value controls the step size of gradient descent. If it's set too high, it could lead to overshooting and instability. If it's set too low, it could lead to a slow model.\n",
    "\n",
    "<br>\n",
    "<img src=\"Images/Gradient_Descent.jpeg\" alt=\"Gradient Descent\" width=\"1200\">\n",
    "\n",
    "Image source: analyticsvidhya.com\n",
    "<br>\n",
    "\n",
    "To perform gradient descent and update our parameters using the functions listed above, we need to find these first:\n",
    "$$\\frac{\\delta L_{CE}}{\\delta W^{[1]}} \\text{, or } dW^{[1]}$$\n",
    "$$\\frac{\\delta L}{\\delta b^{[1]}} \\text{, or } db^{[1]}$$\n",
    "$$\\frac{\\delta L_{CE}}{\\delta W^{[2]}} \\text{, or } dW^{[2]}$$\n",
    "$$\\frac{\\delta L_{CE}}{\\delta b^{[2]}} \\text{, or } db^{[2]}$$\n",
    "\n",
    "Since we're going backwards, we'll calculate $dW^{[2]}$ and $db^{[2]}$ first. To do this, we need to find the derivative of the loss function (${\\delta L_{CE}}$) with respect to the inputs of the output layer $(\\delta Z_i)$. We'll show how this was calculated later, but it is defined as:\n",
    "$$\\frac{\\delta L_{CE}}{\\delta {Z_i}} = {\\hat{y}_i} -{y_i}$$\n",
    "\n",
    "This expression is represented by this line our in backpropagation function:\n",
    "$${{dZ^{[2]}} = {A^{[2]} - Y}}$$\n",
    "\n",
    "Just as defined earlier, ${\\hat{y}_i}$, or $Y$ represents the one-hot encoded labels, and ${y_i}$, or ${A^{[2]}}$ represents the output of the final layer after softmax.\n",
    "\n",
    "Once we have ${dZ^{[2]}}$, we can calculate ${dW^{[2]}}$ and ${db^{[2]}}$:\n",
    "$$dW^{[2]} = \\frac{dZ^{[2]} A^{[1]T}}{m}$$\n",
    "$$dB^{[2]} = \\frac{\\Sigma {dZ^{[2]}}}{m}$$\n",
    "\n",
    "Here, $dW^{[2]}$ and $db^{[2]}$ are divided by m to normalize them.\n",
    "\n",
    "Then, to continue moving backwards through the network, we need to calculate $dW^{[1]}$ and $db^{[1]}$. Similar to calculating ${dW^{[2]}}$ and ${db^{[2]}}$, we'll first calculate:\n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} \\cdot g^{[1]\\prime} (Z^{[1]})$$\n",
    "\n",
    "On the right side of the expression, $g^{[1]\\prime}(Z^{[1]})$ is the derivative of the activation function $g$ (which is ReLU because this is working on the hidden layer where ReLU was used) with respect to $Z$.\n",
    "\n",
    "Recall the ReLU function:\n",
    "$$f(x) = max(0, x)$$\n",
    "\n",
    "The derivative of ReLU (covered earlier) is 1 if x > 0, and 0 if x <= 0.\n",
    "\n",
    "To find $dW^{[1]}$ and $db^{[1]}$, it's similar to finding ${dW^{[2]}}$ and ${db^{[2]}}$. However, instead of $A^{[1]}$, we'll use $X$ which represents the input features:\n",
    "$$dW^{[1]} = \\frac{dZ^{[1]} X^T}{m}$$\n",
    "$$db^{[1]} = \\frac{\\Sigma {dZ^{[1]}}}{m}$$\n",
    "\n",
    "Next, we need to update our weight and bias parameters using the expressions defined in the beginning of this section:\n",
    "$$W^{[1]} = W^{[1]} - \\alpha \\cdot dW^{[1]}$$\n",
    "$$b^{[1]} = b^{[1]} - \\alpha \\cdot db^{[1]}$$\n",
    "$$W^{[2]} = W^{[2]} - \\alpha \\cdot dW^{[2]}$$\n",
    "$$b^{[2]} = b^{[2]} - \\alpha \\cdot db^{[2]}$$\n",
    "\n",
    "That's everything we'll need to carry out gradient descent and train the neural network! This will happen repeatedly until it reaches the desired performance."
   ],
   "metadata": {
    "collapsed": false,
    "id": "KwMCzIHotnb6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    \"\"\"\n",
    "    This function retrieves the predicted labels by finding the index of the maximum value (the highest predicted probability after softmax) in each column of A2\n",
    "\n",
    "    Args:\n",
    "        A2: Activation values of the output layer\n",
    "\n",
    "    Returns:\n",
    "        The predicted labels for the given input.\n",
    "    \"\"\"\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    \"\"\"\n",
    "    This function calculates the accuracy of the predicted labels by comparing them to the true labels\n",
    "\n",
    "    Args:\n",
    "        predictions: Predicted labels for a set of images\n",
    "        Y: True labels for the same set of images\n",
    "\n",
    "    Returns:\n",
    "        The accuracy of the predicted labels, calculated as the ratio of correct predictions to the total number of images\n",
    "    \"\"\"\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def update_param(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    \"\"\"\n",
    "    Updates the parameters using the gradient descent calculations\n",
    "\n",
    "    Args:\n",
    "        W1: Weight values of the hidden layer\n",
    "        b1: Bias values of the hidden layer\n",
    "        W2: Weight values of the output layer\n",
    "        b2: Bias values of the output layer\n",
    "        dW1: Gradients of the cost w.r.t the weights of the hidden layer\n",
    "        db1: Gradients of the cost w.r.t the biases of the hidden layer\n",
    "        dW2: Gradients of the cost w.r.t the weights of the output layer\n",
    "        db2: Gradients of the cost w.r.t the biases of the output layer\n",
    "        alpha: Learning rate\n",
    "\n",
    "    Returns:\n",
    "        W1: Updated weight values of the hidden layer\n",
    "        b1: Updated bias values of the hidden layer\n",
    "        W2: Updated weight values of the output layer\n",
    "        b2: Updated bias values of the output layer\n",
    "    \"\"\"\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def gradient_descent(X, Y, iterations, alpha):\n",
    "    \"\"\"\n",
    "    Implements the gradient descent algorithm, which aims to minimize the loss function by adjusting parameters\n",
    "\n",
    "    Args:\n",
    "        X: Input features\n",
    "        Y: True labels\n",
    "        iterations: Number of iterations gradient descent will run\n",
    "        alpha: Learning rate\n",
    "\n",
    "    Returns:\n",
    "        W1: Final, updated weight values of the hidden layer\n",
    "        b1: Final, updated bias values of the hidden layer\n",
    "        W2: Final, updated weight values of the output layer\n",
    "        b2: Final, updated bias values of the output layer\n",
    "    \"\"\"\n",
    "    W1, b1, W2, b2 = initialize_parameters()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backpropagation(Z1, A1, A2, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_param(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "\n",
    "        if i % 100 == 0: # Print the progress every 100 iterations\n",
    "            one_hot_Y = one_hot_encode(Y)\n",
    "            cost = -np.sum(one_hot_Y * np.log(A2)) / m\n",
    "            predictions = get_predictions(A2)\n",
    "            accuracy_percent = get_accuracy(predictions, Y) * 100\n",
    "\n",
    "            print(f'Iteration: {i}')\n",
    "            print(f'Accuracy: {accuracy_percent:.3f}%')\n",
    "            print(f'Cost: {cost:,.3f}\\n')\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Generates predictions for the given input using the provided weights and biases\n",
    "\n",
    "    Args:\n",
    "        X: Input features\n",
    "        W1: Weight values of the hidden layer\n",
    "        b1: Bias values of the hidden layer\n",
    "        W2: Weight values of the output layer\n",
    "        b2: Bias values of the output layer\n",
    "\n",
    "    Returns:\n",
    "        Predicted output for the given input\n",
    "    \"\"\"\n",
    "    _, _, _, A2 = forward_propagation(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "def test_prediction(index, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Tests a single prediction for the given index using the provided weights and biases by displaying the image, prediction, and label\n",
    "\n",
    "    Args:\n",
    "        index: Index of the example to test.\n",
    "        W1: Weight values of the hidden layer\n",
    "        b1: Bias values of the hidden layer\n",
    "        W2: Weight values of the output layer\n",
    "        b2: Bias values of the output layer\n",
    "    \"\"\"\n",
    "    current_image = X_training[:, index, None]\n",
    "    prediction = make_predictions(X_training[:, index, None], W1, b1, W2, b2)\n",
    "    label = Y_training[index]\n",
    "\n",
    "    is_correct = prediction == label\n",
    "    return current_image, prediction, label, is_correct\n",
    "\n",
    "def display_predictions(n, W1, b1, W2, b2, display_correct=True):\n",
    "    \"\"\"\n",
    "    Collects n correct predictions, by default, and displays them\n",
    "\n",
    "    Args:\n",
    "        n: Number of predictions to collect.\n",
    "        W1: Weight values of the hidden layer\n",
    "        b1: Bias values of the hidden layer\n",
    "        W2: Weight values of the output layer\n",
    "        b2: Bias values of the output layer\n",
    "        display_correct: If True, display correct predictions. If False, display incorrect predictions\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    count = 0\n",
    "    while count < n:\n",
    "        random_index = np.random.randint(0, X_training.shape[1])\n",
    "        current_image, prediction, label, is_correct = test_prediction(random_index, W1, b1, W2, b2)\n",
    "        if is_correct == display_correct:\n",
    "            predictions.append((prediction, label, current_image))\n",
    "            count += 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))\n",
    "\n",
    "    for i, (prediction, label, img) in enumerate(predictions):\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "        axes[row, col].imshow(img.reshape((28, 28)) * 255, cmap='gray')\n",
    "        axes[row, col].set_title(f\"Prediction: {prediction}\\nTrue Label: {label}\")\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    if display_correct:\n",
    "        print('Correct Predictions:\\n')\n",
    "    else:\n",
    "        print('Incorrect Predictions:\\n')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "TVQ0m7Satnb7",
    "ExecuteTime": {
     "end_time": "2023-06-01T00:50:50.533511Z",
     "start_time": "2023-06-01T00:50:50.530644Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# **Results**\n",
    "This is the exciting part! We can finally see our results!\n",
    "\n",
    "First, we have to actually train our model (we'll keep it separated from the other code because it takes a while to run, just in case you wanted to tweak any of the subsequent code). Remember, these results are from the training session, not the testing session. Afterward, we'll benchmark our neural network by running it with the validation data we grabbed from our training data and set aside as our \"testing\" data, and see how the accuracy matches up. Ideally, we want them to be similar, but sometimes it varies.\n",
    "\n",
    "Then, we'll display some correct and incorrect predictions from the training data. I enjoy looking at the incorrect predictions because I try to find what could've caused our model to make a mistake.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "id": "49NSq7Cgtnb7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Accuracy: 6.793%\n",
      "Cost: 4.129\n",
      "\n",
      "Iteration: 100\n",
      "Accuracy: 77.446%\n",
      "Cost: 0.655\n",
      "\n",
      "Iteration: 200\n",
      "Accuracy: 85.027%\n",
      "Cost: 0.464\n",
      "\n",
      "Iteration: 300\n",
      "Accuracy: 87.641%\n",
      "Cost: 0.398\n",
      "\n",
      "Iteration: 400\n",
      "Accuracy: 88.849%\n",
      "Cost: 0.364\n",
      "\n",
      "Iteration: 500\n",
      "Accuracy: 89.641%\n",
      "Cost: 0.341\n",
      "\n",
      "Iteration: 600\n",
      "Accuracy: 90.237%\n",
      "Cost: 0.324\n",
      "\n",
      "Iteration: 700\n",
      "Accuracy: 90.724%\n",
      "Cost: 0.311\n",
      "\n",
      "Iteration: 800\n",
      "Accuracy: 91.054%\n",
      "Cost: 0.300\n",
      "\n",
      "Iteration: 900\n",
      "Accuracy: 91.329%\n",
      "Cost: 0.291\n",
      "\n",
      "Iteration: 1000\n",
      "Accuracy: 91.568%\n",
      "Cost: 0.284\n",
      "\n",
      "Iteration: 1100\n",
      "Accuracy: 91.771%\n",
      "Cost: 0.277\n",
      "\n",
      "Iteration: 1200\n",
      "Accuracy: 91.946%\n",
      "Cost: 0.272\n",
      "\n",
      "Iteration: 1300\n",
      "Accuracy: 92.117%\n",
      "Cost: 0.266\n",
      "\n",
      "Iteration: 1400\n",
      "Accuracy: 92.312%\n",
      "Cost: 0.261\n",
      "\n",
      "Iteration: 1500\n",
      "Accuracy: 92.432%\n",
      "Cost: 0.257\n",
      "\n",
      "Iteration: 1600\n",
      "Accuracy: 92.527%\n",
      "Cost: 0.253\n",
      "\n",
      "Iteration: 1700\n",
      "Accuracy: 92.678%\n",
      "Cost: 0.249\n",
      "\n",
      "Iteration: 1800\n",
      "Accuracy: 92.756%\n",
      "Cost: 0.246\n",
      "\n",
      "Iteration: 1900\n",
      "Accuracy: 92.854%\n",
      "Cost: 0.243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the training data\n",
    "W1, b1, W2, b2 = gradient_descent(X_training, Y_training, 500, 0.25)"
   ],
   "metadata": {
    "id": "Yx7HFUqQtnb7",
    "ExecuteTime": {
     "end_time": "2023-06-01T01:02:03.271644Z",
     "start_time": "2023-06-01T00:59:37.497573Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data Accuracy: 93.600%\n",
      "\n",
      "Correct Predictions:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x600 with 10 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Predictions:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x600 with 10 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the model with the validation data\n",
    "validation_predictions = make_predictions(X_validation, W1, b1, W2, b2)\n",
    "validation_accuracy_percent = get_accuracy(validation_predictions, Y_validation) * 100\n",
    "print(f'Validation Data Accuracy: {validation_accuracy_percent:.3f}%\\n')\n",
    "\n",
    "# Display 10 random correct predictions\n",
    "display_predictions(10, W1, b1, W2, b2)\n",
    "\n",
    "# Display 10 random incorrect predictions\n",
    "display_predictions(10, W1, b1, W2, b2, display_correct=False)"
   ],
   "metadata": {
    "id": "tFuWcEnQtnb7",
    "ExecuteTime": {
     "end_time": "2023-06-01T01:27:08.855994Z",
     "start_time": "2023-06-01T01:27:07.282684Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "# **Clarifications**\n",
    "Although not necessary, I believe some topics needed some further explanation. I've included those in this section.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Derivative Vs. Gradient**\n",
    "These two terms are used everywhere in machine learning. People use them interchangeably (including myself) without any issues, but there technically is a difference.\n",
    "\n",
    "**Derivative**\n",
    "Measures how a function changes as its inputs change. The derivative of a loss function with respect to a parameter provides information about the sensitivity of the cost to changes in the parameters. It's useful when you have only one parameter to optimize.\n",
    "\n",
    "**Gradient**\n",
    "A generalization of the derivative concept to functions with more than one input. It's a vector that points in the direction of steepest ascent of the loss function, with its magnitude representing the rate of increase in that direction. When we compute gradient descent, we find the gradient and move in the opposite direction (the steepest descent). Gradient is computed by taking the partial derivatives of the loss function with respect to all the parameters.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"Images/Gradient.jpeg\" alt=\"Gradient\" width=\"1200\">\n",
    "\n",
    "Image source: statisticshowto.com\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Loss Function**\n",
    "When training a neural network, we don't compute the cost at each step. Furthermore, our code actually doesn't even explicitly calculate the cost at all, it only calculates the derivatives of the loss function with respect to the model's parameters. While we may not explicitly calculate the cost itself, we do calculate something directly derived from the loss function. Therefore, you could say we implicitly calculate the cost.\n",
    "\n",
    "Although factored out in our calculation of cost, the incorrect classes are, in fact, part of the backpropagation calculation and contribute to the gradient descent calculations. Ideally, the model shouldn't have given any probability to the incorrect classes. However, because it's learning, it did, and it needs to be corrected by adjusting the weights and biases. When training a multi-class classifier, the goal is not only to get the correct class right, but also to get the incorrect classes wrong. This ensures that the model is penalized more for making incorrect predictions, and less for making correct predictions.\n",
    "\n",
    "If the model predicts a high probability for the correct class (closer to 1), the difference will be small, leading to a smaller gradient and a smaller update to the parameters. Since the model is making correct predictions, we don't want to adjust the weights too much. If the model predicts a low probability for the correct class or a high probability for an incorrect class, the difference will be large, leading to a larger gradient and a larger update to the parameters. Since the model is making incorrect predictions, we want to change the parameters more to correct these errors.\n",
    "\n",
    "If the model's predicted probability for the correct class increases (i.e. the model is predicting more accurately), then $dZ^{[2]}$ for that class gets smaller (closer to 0) because $dZ^{[2]} = \\text{predicted probability} - 1$ Therefore, $dW^{[2]}$ also gets smaller, since it's directly computed using $dZ^{[2]}$. Conversely, if the model's predicted probability for the correct class decreases, then $dZ^{[2]}$ for that class gets larger (closer to 1). As a result, $dW^{[2]}$ gets larger too. To reinforce this, let's consider an example using the same variables in our code. Assume that we have a neural network with 3 classes, and the outputs from the hidden layer ($A^{[1]}$) and the output layer ($A^{[2]}$) are as follows:\n",
    "$${A^{[1]} = [0.6, 0.4]}$$\n",
    "$$A^{[2]} = [0.1, 0.2, 0.7]$$\n",
    "\n",
    "Let's also say that the true label is Class 3, so the one-hot encoded labels ($\\text{one_hot_Y}$) would be:\n",
    "$$\\text{one_hot_Y} = [0, 0, 1]$$\n",
    "\n",
    "Now, we calculate $dZ2$:\n",
    "$$dZ^{[2]} = A^{[2]} - \\text{one_hot_Y}$$\n",
    "$$= [0.1, 0.2, -0.3]$$\n",
    "\n",
    "$dZ^{[2]}$ is used to calculate $dW^{[2]}$ in the formula $dW^{[2]} = dZ^{[2]} \\cdot A^{[1]T} / m$ during backpropagation. If we have one training example (so $m=1$), this formula simplifies to $dW^{[2]} = dZ^{[2]} \\cdot A^{[1]T}$.\n",
    "\n",
    "Let's calculate $dW^{[2]}$ using the dot product:\n",
    "$$dW^{[2]} = dZ^{[2]} \\cdot A^{[1]T} = [(0.1 \\cdot 0.6 + 0.1 \\cdot 0.4), (0.2 \\cdot 0.6 + 0.2 \\cdot 0.4), (-0.3 \\cdot 0.6 - 0.3 \\cdot 0.4)]$$\n",
    "$$= [0.06, 0.12, -0.18]$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Overestimation and Underestimation**\n",
    "If the predicted probability of a class in $A^{[2]}$ is higher than the actual label, we can say the model overestimates that class. In our example, the model predicted 0.1 for Class 1 and 0.2 for Class 2, but the actual labels for these classes are 0. Therefore, the model overestimated the probabilities of Class 1 and Class 2. $dW^{[2]}$ will also be high, indicating that we need to push the weights in the direction that reduces the cost. Overestimating can lead to incorrect decisions. For example, in a medical diagnosis algorithm, if the model overestimates the correct class (e.g., predicts a disease that a patient does not have), it may lead to unnecessary medical treatments.\n",
    "\n",
    "Overestimating can reduce the discriminative power of the model. When the predicted probability for the correct class is too high, it means the model is assigning high confidence to that class. This may result in the model becoming less sensitive to subtle differences between the correct class and other similar classes. This loss of sensitivity may reduce the model's ability to make nuanced distinctions and affect its overall performance.\n",
    "\n",
    "If the predicted probability of a class in $A^{[2]}$ is lower than the actual label, we can say the model underestimates that class. In our example, the model predicted 0.7 for Class 3, but the actual label for this class is 1. Therefore, the model underestimated the probability of Class 3. $dW^{[2]}$ will also be low, indicating that we need to push the weights in the direction that reduces the cost.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Calibration**\n",
    "\"True probabilities\" refer to the actual frequencies of occurrence of each class within the dataset being modeled. Overestimating can indicate a lack of calibration in the model's predicted probabilities. A well-calibrated model should have predicted probabilities that closely reflect the true probabilities of the classes. A well-calibrated mode is one that provides predicted probabilities that align well with the true probabilities. For example, if the model assigns a predicted probability of $0.8$ to a certain class, it should mean that, on average, this class is expected to occur $80%$ of the time in reality.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "## **Direction of Weight Adjustments**\n",
    "The positive values in $dW^{[2]}$ ($0.06$ and $0.12$ in our example) suggest that decreasing the corresponding weights would decrease the cost. The model's predictions for those classes were too low (underestimated) relative to the true labels. Conversely, the negative values in $dW^{[2]}$ ($-0.18$ in our example) suggest that increasing the corresponding weights would decrease the cost.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "id": "z3S-I2AZtnb8"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
